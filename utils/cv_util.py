from typing import Dict, Tuple

import copy
import numpy as np
import math
import cv2
import scipy.interpolate as si

# =================== Image Format Utilities ===================

def load_image_bgr(image_path: str) -> np.ndarray:
    """
    Load image in BGR format (OpenCV default).
    以BGR格式加载图像(OpenCV默认)
    
    Args:
        image_path: Path to image file / 图像文件路径
        
    Returns:
        Image in BGR format or None if failed / BGR格式的图像，失败则返回None
    """
    return cv2.imread(str(image_path), cv2.IMREAD_COLOR)


def ensure_rgb(img: np.ndarray, source_format: str = 'bgr') -> np.ndarray:
    """
    Ensure image is in RGB format.
    确保图像为RGB格式
    
    Args:
        img: Input image / 输入图像
        source_format: Source color format ('bgr', 'rgb', or 'gray') / 源颜色格式
        
    Returns:
        Image in RGB format / RGB格式的图像
    """
    if img is None:
        raise ValueError("Input image is None")
    
    source_format = source_format.lower()
    
    if source_format == 'rgb':
        return img
    elif source_format == 'bgr':
        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    elif source_format == 'gray' or len(img.shape) == 2:
        return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    else:
        raise ValueError(f"Unknown source format: {source_format}")


def ensure_bgr(img: np.ndarray, source_format: str = 'rgb') -> np.ndarray:
    """
    Ensure image is in BGR format (for OpenCV operations).
    确保图像为BGR格式(用于OpenCV操作)
    
    Args:
        img: Input image / 输入图像
        source_format: Source color format ('bgr', 'rgb', or 'gray') / 源颜色格式
        
    Returns:
        Image in BGR format / BGR格式的图像
    """
    if img is None:
        raise ValueError("Input image is None")
    
    source_format = source_format.lower()
    
    if source_format == 'bgr':
        return img
    elif source_format == 'rgb':
        return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    elif source_format == 'gray' or len(img.shape) == 2:
        return cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    else:
        raise ValueError(f"Unknown source format: {source_format}")


# =================== intrinsics ===================

def parse_fisheye_intrinsics(json_data: dict) -> Dict[str, np.ndarray]:
    """
    Reads camera intrinsics from OpenCameraImuCalibration to opencv format.
    Example:
    {
        "final_reproj_error": 0.17053819312281043,
        "fps": 60.0,
        "image_height": 1080,
        "image_width": 1920,
        "intrinsic_type": "FISHEYE",
        "intrinsics": {
            "aspect_ratio": 1.0026582765352035,
            "focal_length": 420.56809123853304,
            "principal_pt_x": 959.857586309181,
            "principal_pt_y": 542.8155851051391,
            "radial_distortion_1": -0.011968137016185161,
            "radial_distortion_2": -0.03929790706019372,
            "radial_distortion_3": 0.018577224235396064,
            "radial_distortion_4": -0.005075629959840777,
            "skew": 0.0
        },
        "nr_calib_images": 129,
        "stabelized": false
    }
    """
    assert json_data['intrinsic_type'] == 'FISHEYE'
    intr_data = json_data['intrinsics']
    
    # img size
    h = json_data['image_height']
    w = json_data['image_width']

    # pinhole parameters
    f = intr_data['focal_length']
    px = intr_data['principal_pt_x']
    py = intr_data['principal_pt_y']
    
    # Kannala-Brandt non-linear parameters for distortion
    kb8 = [
        intr_data['radial_distortion_1'],
        intr_data['radial_distortion_2'],
        intr_data['radial_distortion_3'],
        intr_data['radial_distortion_4']
    ]

    opencv_intr_dict = {
        'DIM': np.array([w, h], dtype=np.int64),
        'K': np.array([
            [f, 0, px],
            [0, f, py],
            [0, 0, 1]
        ], dtype=np.float64),
        'D': np.array([kb8]).T
    }
    print(opencv_intr_dict)
    return opencv_intr_dict


def convert_fisheye_intrinsics_resolution(
        opencv_intr_dict: Dict[str, np.ndarray], 
        target_resolution: Tuple[int, int]
        ) -> Dict[str, np.ndarray]:
    """
    Convert fisheye intrinsics parameter to a different resolution,
    assuming that images are not cropped in the vertical dimension,
    and only symmetrically cropped/padded in horizontal dimension.
    """
    iw, ih = opencv_intr_dict['DIM']
    # print(f"iw: {iw}, ih: {ih}")
    iK = opencv_intr_dict['K']
    ifx = iK[0,0]
    ify = iK[1,1]
    ipx = iK[0,2]
    ipy = iK[1,2]

    ow, oh = target_resolution
    ofx = ifx / ih * oh
    ofy = ify / ih * oh
    opx = (ipx - (iw / 2)) / ih * oh + (ow / 2)
    opy = ipy / ih * oh
    oK = np.array([
        [ofx, 0, opx],
        [0, ofy, opy],
        [0, 0, 1]
    ], dtype=np.float64)

    out_intr_dict = copy.deepcopy(opencv_intr_dict)
    out_intr_dict['DIM'] = np.array([ow, oh], dtype=np.int64)
    out_intr_dict['K'] = oK
    return out_intr_dict


class FisheyeRectConverter:
    def __init__(self, K, D, DIM, out_size, out_fov):
        out_size = np.array(out_size)
        # vertical fov
        out_f = (out_size[1] / 2) / np.tan(out_fov/180*np.pi/2)
        out_K = np.array([
            [out_f, 0, out_size[0]/2],
            [0, out_f, out_size[1]/2],
            [0, 0, 1]
        ], dtype=np.float32)
        map1, map2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), out_K, out_size, cv2.CV_16SC2)

        self.map1 = map1
        self.map2 = map2
    
    def forward(self, img):
        rect_img = cv2.remap(img, 
            self.map1, self.map2,
            interpolation=cv2.INTER_AREA, 
            borderMode=cv2.BORDER_CONSTANT)
        return rect_img

# ================= ArUcO tag =====================
def parse_aruco_config(aruco_config_dict: dict):
    """
    example:
    aruco_dict:
        predefined: DICT_4X4_50
    marker_size_map: # all unit in meters
        default: 0.15
        12: 0.2
    """
    aruco_dict = get_aruco_dict(**aruco_config_dict['aruco_dict'])

    n_markers = len(aruco_dict.bytesList)
    marker_size_map = aruco_config_dict['marker_size_map']
    default_size = marker_size_map.get('default', None)
    
    out_marker_size_map = dict()
    for marker_id in range(n_markers):
        size = default_size
        if marker_id in marker_size_map:
            size = marker_size_map[marker_id]
        out_marker_size_map[marker_id] = size
    
    result = {
        'aruco_dict': aruco_dict,
        'marker_size_map': out_marker_size_map
    }
    return result


def get_aruco_dict(predefined:str
                   ) -> cv2.aruco.Dictionary:
    return cv2.aruco.getPredefinedDictionary(
        getattr(cv2.aruco, predefined))

def detect_localize_aruco_tags(
        img: np.ndarray,
        aruco_dict: cv2.aruco.Dictionary,
        marker_size_map: Dict[int, float],
        fisheye_intr_dict: Dict[str, np.ndarray],
        refine_subpix: bool = True,
        # --- Exposed ArUco detector parameters (with sensible defaults) ---
        adaptiveThreshWinSizeMin: int = 3,
        adaptiveThreshWinSizeMax: int = 23,
        adaptiveThreshWinSizeStep: int = 10,
        adaptiveThreshConstant: float = 8.0,
        minMarkerPerimeterRate: float = 0.03,
        maxMarkerPerimeterRate: float = 4.0,
        polygonalApproxAccuracyRate: float = 0.03):
    """
    Detect and localize ArUco markers in an image.
    检测并定位图像中的ArUco标记
    
    This function accepts images in any format (BGR, RGB, or grayscale).
    OpenCV's ArUco detector internally converts color images to grayscale,
    so there's no accuracy difference between formats.
    该函数接受任何格式的图像(BGR、RGB或灰度)。
    OpenCV的ArUco检测器内部会将彩色图像转换为灰度图，
    因此不同格式之间的精度没有差异。
    
    Args:
        img: Input image (BGR, RGB, or grayscale) / 输入图像(BGR、RGB或灰度)
        aruco_dict: ArUco dictionary / ArUco字典
        marker_size_map: Map of marker IDs to physical sizes (meters) / 标记ID到物理尺寸的映射(米)
        fisheye_intr_dict: Fisheye camera intrinsics / 鱼眼相机内参
        refine_subpix: Enable subpixel corner refinement / 启用亚像素角点精化
        
    Returns:
        Dictionary mapping detected marker IDs to their pose and corners
        检测到的标记ID到其姿态和角点的字典映射
        {marker_id: {'rvec': ..., 'tvec': ..., 'corners': ...}}
    """
    # Validate input image format
    # 验证输入图像格式
    if img is None:
        raise ValueError("Input image is None")
    if len(img.shape) not in [2, 3]:
        raise ValueError(f"Expected 2D (grayscale) or 3D (color) image, got shape {img.shape}")
    if len(img.shape) == 3 and img.shape[2] not in [3, 4]:
        raise ValueError(f"Expected 3 or 4 channels for color image, got {img.shape[2]}")
    
    K = fisheye_intr_dict['K']
    D = fisheye_intr_dict['D']

    # Create and configure ArUco detector parameters
    param = cv2.aruco.DetectorParameters()
    # threshold window sizes
    param.adaptiveThreshWinSizeMin = adaptiveThreshWinSizeMin
    param.adaptiveThreshWinSizeMax = adaptiveThreshWinSizeMax
    param.adaptiveThreshWinSizeStep = adaptiveThreshWinSizeStep
    # threshold constant (lower if marker对比度低, higher if噪声多)
    param.adaptiveThreshConstant = adaptiveThreshConstant
    # marker size相关
    param.minMarkerPerimeterRate = minMarkerPerimeterRate
    param.maxMarkerPerimeterRate = maxMarkerPerimeterRate
    # 轮廓多边形拟合精度
    param.polygonalApproxAccuracyRate = polygonalApproxAccuracyRate

    if refine_subpix:
        param.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_SUBPIX
    
    # Use new OpenCV 4.7+ ArUco API
    detector = cv2.aruco.ArucoDetector(aruco_dict, param)
    corners, ids, rejectedImgPoints = detector.detectMarkers(img)
    
    if ids is None or len(corners) == 0:
        return dict()

    tag_dict = dict()
    for this_id, this_corners in zip(ids, corners):
        this_id = int(this_id[0])
        if this_id not in marker_size_map:
            continue
        
        marker_size_m = marker_size_map[this_id]
        if marker_size_m is None:
            continue
        undistorted = cv2.fisheye.undistortPoints(this_corners, K, D, P=K)
        
        # Create 3D object points for ArUco marker (standard order: TL, TR, BR, BL)
        object_points = np.array([
            [-marker_size_m/2, marker_size_m/2, 0],   # Top-left
            [marker_size_m/2, marker_size_m/2, 0],    # Top-right
            [marker_size_m/2, -marker_size_m/2, 0],   # Bottom-right
            [-marker_size_m/2, -marker_size_m/2, 0]   # Bottom-left
        ], dtype=np.float32)
        
        # Use solvePnP instead of estimatePoseSingleMarkers
        success, rvec, tvec = cv2.solvePnP(
            object_points, undistorted.reshape(-1, 2), K, np.zeros((1,5)))
        
        if success:
            tag_dict[this_id] = {
                'rvec': rvec.squeeze(),
                'tvec': tvec.squeeze(),
                'corners': this_corners.squeeze()
            }
    return tag_dict

def get_gripper_width(tag_dict, left_id, right_id): 
    left_x = None
    if left_id in tag_dict:
        tvec = tag_dict[left_id]['tvec']
        left_x = tvec[0]

    right_x = None
    if right_id in tag_dict:
        tvec = tag_dict[right_id]['tvec']
        right_x = tvec[0]

    width = None
    if (left_x is not None) and (right_x is not None):
        width = np.abs(right_x - left_x)
        
    return width

def draw_fisheye_mask(img, radius=None, center=None, fill_color=(255, 255, 255)):
    h, w = img.shape[:2]
    
    # Default center is image center
    if center is None:
        cx, cy = w // 2, h // 2
    else:
        cx, cy = center
    
    # Default radius is a reasonable size (adjustable based on specific fisheye lens)
    if radius is None:
        radius = min(w, h) // 2 - 50  # Leave some margin to avoid cutting effective area
    
    # Create circular mask: 255 inside circle, 0 outside
    mask = np.zeros((h, w), dtype=np.uint8)
    cv2.circle(mask, (cx, cy), radius, 255, -1)
    mask3 = cv2.merge([mask, mask, mask])  # Convert to 3 channels
    
    # Create fill background
    fill_bg = np.full_like(img, fill_color, dtype=img.dtype)
    
    # Keep original image inside circle, use fill color outside
    result = np.where(mask3 == 255, img, fill_bg)
    
    return result

def inpaint_tag(img, corners, tag_scale, n_samples=16):
    # scale corners with respect to geometric center
    center = np.mean(corners, axis=0)
    scaled_corners = tag_scale * (corners - center) + center
    
    # sample pixels on the boundary to obtain median color
    sample_points = si.interp1d(
        [0,1,2,3,4], list(scaled_corners) + [scaled_corners[0]], 
        axis=0)(np.linspace(0,4,n_samples)).astype(np.int32)
    sample_colors = img[
        np.clip(sample_points[:,1], 0, img.shape[0]-1), 
        np.clip(sample_points[:,0], 0, img.shape[1]-1)
    ]
    median_color = np.median(sample_colors, axis=0).astype(img.dtype)
    
    # draw tag with median color
    img = cv2.fillPoly(
        img, scaled_corners[None,...].astype(np.int32), 
        color=median_color.tolist())
    return img

# =========== other utils ====================
def get_fisheye_image_transform(in_res, out_res, crop_ratio:float = 1.0, bgr_to_rgb: bool=False):
    iw, ih = in_res
    ow, oh = out_res
    ch = round(ih * crop_ratio)
    cw = round(ih * crop_ratio / oh * ow)
    interp_method = cv2.INTER_AREA

    w_slice_start = (iw - cw) // 2
    w_slice = slice(w_slice_start, w_slice_start + cw)
    h_slice_start = (ih - ch) // 2
    h_slice = slice(h_slice_start, h_slice_start + ch)
    c_slice = slice(None)
    if bgr_to_rgb:
        c_slice = slice(None, None, -1)

    def transform(img: np.ndarray):
        assert img.shape == ((ih,iw,3))
        # crop
        img = img[h_slice, w_slice, c_slice]
        # resize
        img = cv2.resize(img, out_res, interpolation=interp_method)
        return img
    
    return transform

# TODO: Directly copied, but definitely not reasonable, needs modification.
def get_tactile_image_transform(in_res, out_res, crop_ratio:float = 1.0, bgr_to_rgb: bool=False):
    iw, ih = in_res
    ow, oh = out_res
    ch = round(ih * crop_ratio)
    cw = round(ih * crop_ratio / oh * ow)
    interp_method = cv2.INTER_AREA

    w_slice_start = (iw - cw) // 2
    w_slice = slice(w_slice_start, w_slice_start + cw)
    h_slice_start = (ih - ch) // 2
    h_slice = slice(h_slice_start, h_slice_start + ch)
    c_slice = slice(None)
    if bgr_to_rgb:
        c_slice = slice(None, None, -1)

    def transform(img: np.ndarray):
        # Flexible shape check - allow for different input resolutions
        if img.shape != (ih, iw, 3):
            print(f"Warning: Expected image shape ({ih}, {iw}, 3), got {img.shape}. Adjusting...")
            # Simply resize without cropping if dimensions don't match
            img = cv2.resize(img, out_res, interpolation=interp_method)
            return img
        
        # crop
        img = img[h_slice, w_slice, c_slice]
        # resize
        img = cv2.resize(img, out_res, interpolation=interp_method)
        return img
    
    return transform